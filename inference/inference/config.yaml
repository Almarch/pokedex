llm:
  model: "Qwen/Qwen3-8B"
  quantized: true
  gpu_memory_utilization: 0.8

generate:
  temperature: 0.35
  max_tokens: 256
  top_p: 0.9
  top_k: 50

rerank:
  batch_size: 4

embed:
  model: "Qwen/Qwen3-Embedding-0.6B"
  asymmetric: true
  batch_size: 16
  
cache: "/cache"


